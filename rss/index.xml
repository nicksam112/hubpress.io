<?xml version="1.0" encoding="UTF-8"?><rss xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom" version="2.0"><channel><title><![CDATA[Nick Samoray's Blog]]></title><description><![CDATA[Posts and Projects]]></description><link>https://nicksam112.github.io</link><image><url>http://imgur.com/59rv0VP.jpg</url><title>Nick Samoray&apos;s Blog</title><link>https://nicksam112.github.io</link></image><generator>RSS for Node</generator><lastBuildDate>Wed, 09 Aug 2017 21:07:50 GMT</lastBuildDate><atom:link href="https://nicksam112.github.io/rss/" rel="self" type="application/rss+xml"/><ttl>60</ttl><item><title><![CDATA[Natural Evolution Strategies in Keras]]></title><description><![CDATA[<div id="preamble">
<div class="sectionbody">
<div class="paragraph">
<p>This blog post is written to serve as background and explaination for my implementation of <a href="https://gist.github.com/nicksam112/00e9638c0efad1adac878522cf172484">Natural Evolution Strategies in Keras</a></p>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_background">Background</h2>
<div class="sectionbody">
<div class="paragraph">
<p>If you&#8217;re familiar with neural networks, you&#8217;ve probably run into the concept of backpropagation. It&#8217;s the most common method of training neural networks and for good reason, it&#8217;s extremely effective for complex tasks such as image recognition.</p>
</div>
<div class="paragraph">
<p>But for tasks such as reinforcement learning there may be other feasible methods, at least that&#8217;s what OpenAI is saying in their <a href="https://blog.openai.com/evolution-strategies/">recent blog post</a></p>
</div>
<div class="paragraph">
<p>The gist of it is that while reinforcement learning has seen incredible advances in recent years, with advances such as Deep Q Learning, A3C, and more, it&#8217;s still incredibly difficult to scale. Training typically is done on a single machine due to the communication costs of constantly sharing massive weight files between processes and machines, with asynchronous methods like A3C limited to a single CPU due to these communication costs.</p>
</div>
<div class="paragraph">
<p>That&#8217;s where OpenAI is saying Natural Evolution Strategies come into play, since you only have to share a couple of scalars instead of all the weights, it becomes far easier to scale it up to multiple machines or a cluster.</p>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_what_are_evolution_strategies">What are Evolution Strategies?</h2>
<div class="sectionbody">
<div class="paragraph">
<p>Evolution strategies, sometimes known as genetic algorithms, are a broad category that need more than a single blog post to go over, but the basics for this implementation are fairly simple.</p>
</div>
<div class="olist arabic">
<ol class="arabic">
<li>
<p>Create a bunch of models with weights randomly generated on a normal distribution</p>
</li>
<li>
<p>Evaluate all the models on the environment you&#8217;re trying to solve</p>
</li>
<li>
<p>Collect all the rewards, and weight the weights (eugh) according to their performance</p>
</li>
<li>
<p>Sum up the weights and update a central model that will serve as the base for the next round of randomly generated weights</p>
</li>
</ol>
</div>
<div class="paragraph">
<p>You can see why they&#8217;re called "Evolution Strategies", at it&#8217;s core you&#8217;re throwing a bunch of models into the environment, picking the best, and repeating.</p>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_why_use_them">Why Use Them?</h2>
<div class="sectionbody">
<div class="paragraph">
<p>As mentioned in the background they&#8217;re easy to scale due to the low communication costs. Instead of having to share the weights that they&#8217;re using, each worker only has to let the central parameter server which random seed they&#8217;re using, what iteration they&#8217;re on, and how well that model did. With just the random seed and the iteration, the central parameter server can recreate the weights the worker used.</p>
</div>
<div class="paragraph">
<p>While on a single machine it might be slower than using Deep Q Learning, A3C, or something else, it really shines when used on a cluster. OpenAI in their blog post mentioned they were able to solve complex environments (such as teaching a model of a human to walk!) in just ten minutes with a cluster.</p>
</div>
<div class="paragraph">
<p>Another benefit is that you only have to do a forward pass on the models, there&#8217;s no backpropagation so no need to calculate gradients. That means less math and faster evaluation of a models performance.</p>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_when_not_to_use_them">When Not to Use Them</h2>
<div class="sectionbody">
<div class="paragraph">
<p>Evolution strategies are not without their drawbacks however, while they can scale, they struggle with larger networks. Reinforcement learning networks tend to be relatively simple, usually only consisting of two hidden layers with less than 512 nodes per layer, and thus it makes sense for evolution strategies.</p>
</div>
<div class="paragraph">
<p>But complex networks such as image recognition networks have millions of parameters to adjust, making backpropagation a far more feasible solution.</p>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_conclusion">Conclusion</h2>
<div class="sectionbody">
<div class="paragraph">
<p>Evolutionary strategies have been on the wayside for years due to backpropagations success, however that&#8217;s not to say they don&#8217;t have their merits and applications. OpenAI has been able to show that they&#8217;re not only feasible, but far more effective than traditional backpropagation methods for reinforcement learning.</p>
</div>
<div class="paragraph">
<p>You can check out my gist linked at the start of this article if you want to try it out for yourself, while it only works on a single machine negating the major benefit to using evolution strategies, it serves as a nice proof of concept</p>
</div>
<div class="paragraph">
<p>-Nick</p>
</div>
</div>
</div>]]></description><link>https://nicksam112.github.io/2017/08/02/Natural-Evolution-Strategies-in-Keras.html</link><guid isPermaLink="true">https://nicksam112.github.io/2017/08/02/Natural-Evolution-Strategies-in-Keras.html</guid><category><![CDATA[Keras]]></category><category><![CDATA[Neural Networks]]></category><category><![CDATA[Evolution Strategies]]></category><dc:creator><![CDATA[Nick Samoray]]></dc:creator><pubDate>Wed, 02 Aug 2017 00:00:00 GMT</pubDate></item><item><title><![CDATA[Making a Self Driving Robot with Just a Webcam]]></title><description><![CDATA[<div id="preamble">
<div class="sectionbody">
<div class="paragraph">
<p>Before we start, <a href="https://gfycat.com/DecentAbsoluteFlies">here&#8217;s a gyf of it in action</a></p>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_background">Background</h2>
<div class="sectionbody">
<div class="paragraph">
<p>For my internship at Booz Allen Hamilton, one of my tasks was to find a way to navigate a robot with a sensor package mounted on it throughout a building. The problem? I couldn&#8217;t modify the existing building infrastructure, so no guiding magnets or painting lines, and it had to be done cheaply.</p>
</div>
<div class="paragraph">
<p>With a budget of only a few thousand for the entire project, and the interior robot being only a third of the overall solution, there wasn&#8217;t a lot of wiggle room.</p>
</div>
<div class="paragraph">
<p>The solution I came up with? A roomba, a rasberry pi, and deep learning.</p>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_the_hardware_solution">The Hardware Solution</h2>
<div class="sectionbody">
<div class="paragraph">
<p>I settled on a roomba due to a few reasons, it was cheap (about 200 bucks), it was large enough to mount plenty of sensors, and it was easy enough to control with a raspberry pi.</p>
</div>
<div class="paragraph">
<p>The shopping list consisted of:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>Roomba</p>
</li>
<li>
<p>Serial to USB cable</p>
</li>
<li>
<p>Raspberry Pi</p>
</li>
<li>
<p>Cheapest webcam we could find</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>Total cost was about 300 bucks, but now came the question of how to actually make it autonomous. I ultimately decided on a visual input based navigation system. Due to cost, a fancy LIDAR system wasn&#8217;t much of an option, and I wanted to see just how well I could get it to navigate with just a webcam.</p>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_the_software_solution">The Software Solution</h2>
<div class="sectionbody">
<div class="paragraph">
<p>On the software end I used Keras and decided to treat it as a traditional image classification problem with three different classes, left, right, and forward. However there was an issue of processing power.</p>
</div>
<div class="paragraph">
<p>Initially I wanted to mount the laptop on top of the roomba and use its webcam and processing power. However due to the need to mount all the other sensors on the roomba, that was out of the picture. Thus I needed the model to fit on a raspberry pi and be able to run quickly enough to effectively navigate. I settled on 10 FPS which allowed me to use a model with about 1.5 million parameters in order to navigate.</p>
</div>
<div class="paragraph">
<p>With only 1.5 million parameters, I settled on the following model architecture</p>
</div>
<table class="tableblock frame-all grid-all" style="width: 50%;">
<colgroup>
<col style="width: 50%;">
<col style="width: 50%;">
</colgroup>
<tbody>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">3x94x126</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Input</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">3x3x16</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">2D Convolution</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">3x3x32</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">2D Convolution</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">3x3x64</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">2D Convolution</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">3x3x128</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">2D Convolution</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">512</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Dense</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">3</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Output</p></td>
</tr>
</tbody>
</table>
<div class="paragraph">
<p>This came out to 1.672 million parameters and was able to run at at least 10 FPS on the raspberry pi</p>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_training">Training</h2>
<div class="sectionbody">
<div class="paragraph">
<p>Training the bot was fairly straight forward, I wrote a python script that would read keyboard input and take a picture every 1/10 of a second which was then saved in the corresponding folder (left, right, forward), and ran it around the office several times. Care was taken to keep it centered in hallways and aiming it towards the next doorway.</p>
</div>
<div class="paragraph">
<p>Next I ran it through the office again, this time picking it up and moving it off center or at odd angles, then guiding it back on to the right track. This was essential for making sure the bot would actually know hwo to react if it ran off course for whatever reason.</p>
</div>
<div class="paragraph">
<p>The model was then trained on an Amazon EC2 p2.xlarge isntance for about 8 hours until its accuracy was hovering around 95%.</p>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_result">Result</h2>
<div class="sectionbody">
<div class="paragraph">
<p>The roomba was able to (somewhat) successfully navigate through portions of the office. It was able to look for doorways, stay centered in hallways, however issues popped up in certain areas. If a doorway wasn&#8217;t visible, or there was a lot of visual noise (people walking around, etc), the bot could struggle. However as an initial first step I think it&#8217;s encouraging</p>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_future_directions">Future Directions</h2>
<div class="sectionbody">
<div class="paragraph">
<p>Two interesting directions come to mind when considering the project, imitation learning and reinforcement learning.</p>
</div>
<div class="paragraph">
<p>A lot of work has recently come out in regards to imitation learning for robots using deep learning, teams like OpenAI have been able to teach <a href="https://blog.openai.com/robots-that-learn/">robots how to perform a task just by observing a human doing it</a>. Modifying this to instead teach a robot to navigate through a certain building after walking it through it once could be feasible and would be incredibly interesting</p>
</div>
<div class="paragraph">
<p>Reinforcement learning is another direction that comes to mind, with the front bumper the roomba is able to detect when it bumps into an object. If this could be tied to a negative reward, and have going into a new room being tagged as a positive reward, it might be possible to have it teach itself how to get around a building. The positive rewards could be determined by RFID tags on the ground and a scanner on the bot, the more it hits, and the faster it hits them, the better the reward. That however requries modifying the existing infrastructure, but perhaps for a future project.</p>
</div>
<div class="paragraph">
<p>-Nick</p>
</div>
</div>
</div>]]></description><link>https://nicksam112.github.io/2017/07/15/Making-a-Self-Driving-Robot-with-Just-a-Webcam.html</link><guid isPermaLink="true">https://nicksam112.github.io/2017/07/15/Making-a-Self-Driving-Robot-with-Just-a-Webcam.html</guid><category><![CDATA[Deep Learning]]></category><category><![CDATA[Autonomous Navigation]]></category><category><![CDATA[Keras]]></category><dc:creator><![CDATA[Nick Samoray]]></dc:creator><pubDate>Sat, 15 Jul 2017 00:00:00 GMT</pubDate></item></channel></rss>